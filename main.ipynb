{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Export/Save the Trained Model\n",
    "Once you have trained a model, the first step is to save it in a format that can be loaded during deployment. Different libraries have different ways of saving models.\n",
    "\n",
    "Let’s train an example model on the iris dataset and save it using the pickle method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_model.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# load dataset\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n",
    "\n",
    "# train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# save the model using joblib\n",
    "joblib.dump(model, 'rf_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Writing a Wrapper Function to Load the Model\n",
    "For deployment, you’ll need a function that loads the model and makes predictions. \n",
    "This wrapper function will be exposed through an API or used in the production environment. Here’s how to write the wrapper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load the saved model\n",
    "def load_model():\n",
    "    model = joblib.load('rf_model.pkl')\n",
    "    return model\n",
    "\n",
    "# function to make predictions\n",
    "def predict(input_data):\n",
    "    model = load_model()\n",
    "    prediction = model.predict(input_data)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Setting Up an API to Serve the Model\n",
    "To serve your model in production, you can expose it via an API. Flask or FastAPI is often used to create REST APIs for this purpose.\n",
    "\n",
    "Here’s how to set up an API to serve your model (please make sure to write this piece of code in a separate Python file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# load the model\n",
    "model = joblib.load('rf_model.pkl')\n",
    "\n",
    "# define a route for making predictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()  # Get input data from POST request\n",
    "    input_data = np.array(data['input']).reshape(1, -1)  # Reshape input\n",
    "    prediction = model.predict(input_data)\n",
    "    return jsonify({'prediction': int(prediction[0])})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "So, this is how we can package Machine Learning models in the form of APIs. Model packaging is an essential step in the machine learning deployment process, where the trained model is prepared in a format that can be easily deployed and integrated into production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
